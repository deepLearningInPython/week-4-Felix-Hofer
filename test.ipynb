{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f5bb84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8f20b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "text = \"The quick brown fox jumps over the lazy dog!\"\n",
    "punct = \".,!?;:'\\\"()-[]{}<>/*\\\\\"\n",
    "# Write a list comprehension to tokenize the text and remove punctuation\n",
    "tokens = [word.strip(punct) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd7cae5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# Expected output: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "15a46b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string: str) -> list:\n",
    "    punct = \".,!?;:'\\\"()-[]{}<>/*\\\\\"\n",
    "    tokens = [word.strip(punct) for word in string.split()]\n",
    "    tokens = list(set([word.lower() for word in tokens]))\n",
    "    tokens.sort()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "600f2307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brown', 'dog', 'fox', 'jumps', 'lazy', 'over', 'quick', 'the']"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b010f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "jumps\n",
      "over\n",
      "the\n",
      "lazy\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "freq = {token.lower(): 0 for token in tokens}\n",
    "for word in tokens:\n",
    "    word = word.lower()\n",
    "    print(word)\n",
    "    freq[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7fa997ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 2, 'quick': 1, 'brown': 1, 'fox': 1, 'jumps': 1, 'over': 1, 'lazy': 1, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d90e9d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Define a function that takes a string and an integer k, and returns a dictionary with\n",
    "#   the token frequencies of only those tokens that occur more than k times in the string.\n",
    "\n",
    "# Your code here:\n",
    "# -----------------------------------------------\n",
    "def token_counts(string: str, k: int = 1) -> dict:\n",
    "    punct = \".,!?;:'\\\"()-[]{}<>/*\\\\\"\n",
    "    tokens = [word.strip(punct) for word in string.split()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens.sort()\n",
    "    freq = {token.lower(): 0 for token in tokens}\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        freq[word] += 1\n",
    "    counts = {}\n",
    "    for key, value in freq.items():\n",
    "        if freq[key] > k:\n",
    "            counts[key] = value\n",
    "    \n",
    "    return counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2aeaf235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_hist = {'the': 2, 'quick': 1, 'brown': 1, 'fox': 1, 'jumps': 1, 'over': 1, 'lazy': 1, 'dog': 1}\n",
    "all(text_hist[key] == value for key, value in token_counts(text).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5cddc532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brown', 'dog', 'fox', 'jumps', 'lazy', 'over', 'quick', 'the', 'the']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f3be58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {token: i for i, token in enumerate(tokenize(text))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b8535234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'brown', 1: 'dog', 2: 'fox', 3: 'jumps', 4: 'lazy', 5: 'over', 6: 'quick', 7: 'the'} {'brown': 0, 'dog': 1, 'fox': 2, 'jumps': 3, 'lazy': 4, 'over': 5, 'quick': 6, 'the': 7}\n"
     ]
    }
   ],
   "source": [
    "print(id_to_token, token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b18316fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_token = {idx: token for idx, token in enumerate(tokenize(text))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "15bbc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests: \n",
    "# test 1\n",
    "assert id_to_token[token_to_id['dog']] == 'dog'\n",
    "# test 2\n",
    "assert token_to_id[id_to_token[4]] == 4\n",
    "# test 3\n",
    "assert all(id_to_token[token_to_id[key]]==key for key in token_to_id) and all(token_to_id[id_to_token[k]]==k for k in range(len(token_to_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "8d7ec05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocabulary_map(documents: list) -> tuple:\n",
    "    unique_tokens = []\n",
    "    for doc in documents:\n",
    "        for t in tokenize(doc):\n",
    "            unique_tokens.append(t)\n",
    "    unique_tokens.sort()\n",
    "    token_to_id = {token: i for i, token in enumerate(unique_tokens)}\n",
    "    id_to_token = {idx: token for idx, token in enumerate(unique_tokens)}\n",
    "    return token_to_id, id_to_token\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "3a8ae2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2i, i2t = make_vocabulary_map([text, 'What a luck we had today!'])\n",
    "all(i2t[t2i[tok]] == tok for tok in t2i) # should be True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "16475602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_encode(documents: list) -> list:\n",
    "    def tokenizer(string: str) -> list:\n",
    "        punct = \".,!?;:'\\\"()-[]{}<>/*\\\\\"\n",
    "        tokens = [word.strip(punct) for word in string.split()]\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        return tokens\n",
    "    t2i, i2t = make_vocabulary_map(documents)\n",
    "    enc = []\n",
    "    tok = [tokenizer(doc) for doc in documents]\n",
    "    print(tok)\n",
    "    for l in tok:\n",
    "        enc.append([t2i[word] for word in l])\n",
    "    return enc, t2i, i2t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "e3ec69f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['what', 'a', 'luck', 'we', 'had', 'today']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[10, 9, 1, 3, 5, 8, 10, 6, 2], [13, 0, 7, 12, 4, 11]],\n",
       " {'a': 0,\n",
       "  'brown': 1,\n",
       "  'dog': 2,\n",
       "  'fox': 3,\n",
       "  'had': 4,\n",
       "  'jumps': 5,\n",
       "  'lazy': 6,\n",
       "  'luck': 7,\n",
       "  'over': 8,\n",
       "  'quick': 9,\n",
       "  'the': 10,\n",
       "  'today': 11,\n",
       "  'we': 12,\n",
       "  'what': 13},\n",
       " {0: 'a',\n",
       "  1: 'brown',\n",
       "  2: 'dog',\n",
       "  3: 'fox',\n",
       "  4: 'had',\n",
       "  5: 'jumps',\n",
       "  6: 'lazy',\n",
       "  7: 'luck',\n",
       "  8: 'over',\n",
       "  9: 'quick',\n",
       "  10: 'the',\n",
       "  11: 'today',\n",
       "  12: 'we',\n",
       "  13: 'what'})"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_encode([text, 'What a luck we had today!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "cf8c78d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['what', 'a', 'luck', 'we', 'had', 'today']]\n"
     ]
    }
   ],
   "source": [
    "enc, t2i, i2t = tokenize_and_encode([text, 'What a luck we had today!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "e58a9be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\" | \".join([\" \".join(i2t[i] for i in e) for e in enc]) == 'the quick brown fox jumps over the lazy dog | what a luck we had today'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "8abca52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "69364b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(sigmoid(np.log([1, 1/3, 1/7])) == np.array([1/2, 1/4, 1/8]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
